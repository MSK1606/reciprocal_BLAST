{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schmelling,Nicolas \n",
      "Last updated: 29/09/2015 \n",
      "\n",
      "CPython 2.7.9\n",
      "IPython 3.0.0\n",
      "\n",
      "numpy 1.9.2\n",
      "pandas 0.15.2\n",
      "scipy 0.15.1\n",
      "biopython 1.65\n"
     ]
    }
   ],
   "source": [
    "%watermark -a Schmelling,Nicolas -u -d -v -p numpy,pandas,scipy,biopython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Any comments and suggestions or questions?     \n",
    "Please feel free to contact me via [twitter](https://twitter.com/bio_mediocre) or [email](mailto:schmelli@msu.edu).\n",
    "\n",
    "---\n",
    "\n",
    "<a id='Content'></a>\n",
    "-----\n",
    "\n",
    "#Content#\n",
    "---\n",
    "\n",
    "| [Biopython BLAST](#BLAST) | [Download Information from GenBank](#genbank) | [Command line BLAST](#BLAST back) | [Filtering of the reciprocal BLAST](#filter) | [Merging and final filtering](#CSV) |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from Bio.Blast import NCBIWWW\n",
    "from Bio.Blast import NCBIXML\n",
    "from Bio import SeqIO\n",
    "from Bio import Phylo\n",
    "from Bio import AlignIO\n",
    "from Bio import Entrez\n",
    "from urllib2 import HTTPError\n",
    "import glob\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###BLAST###\n",
    "<a id='BLAST'></a>\n",
    "-----\n",
    "---\n",
    "\n",
    "Before we get started with our BLAST run, we need a fasta file with our sequences. So either you copy/paste your sequences of interest in a new fasta file or you download a file that contains all your sequences. In my case I needed to get all sequences and create a new fasta file.      \n",
    "\n",
    "Your fasta file should look something like this:          \n",
    "\n",
    "`> fasta sequence header`     \n",
    "`sequence e.g. MASJINDHASDINEIHGNISD`      \n",
    "`> next sequence header`      \n",
    "`next sequence e.g. MINASIDJGHIENGISJDA`     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I used the Biopython package for all BLAST related analysis and coding. For more information check www.biopython.org.\n",
    "# First we read our FASTA file sequence by sequence. The SeqIO.parse function needs only a file name, but to make sure it reads\n",
    "# the file correct, we specify the file format.\n",
    "for seq_record in SeqIO.parse('clock_proteins_Arabidopsis.faa', format='fasta'):\n",
    "    \n",
    "    # Next we send our sequence to NCBI server and use the BLAST online version. \n",
    "    # NCBIWWW takes a program argument: In our case blastp, b/c we blast protein against protein. \n",
    "    # a database argument: In our case nr, b/c we want to blast against all non-redundant protein sequences.\n",
    "    # a format argument: Just to make sure BLAST reads our sequences correctly.\n",
    "    # and various additional arguments (optinal): expect = e-value (cut off value, for less similarity), \n",
    "                                                # hitlist_size = no. of maximum hits\n",
    "                                                # matrix_name and word_size are the default values.\n",
    "    result_handle = NCBIWWW.qblast('blastp', 'nr', seq_record.format('fasta'), expect=0.00001,\n",
    "                                   hitlist_size=10000, matrix_name='BLOSUM62', word_size=3)\n",
    "    \n",
    "    # We save the BLAST result for every sequence in a new XML file, for later parsing.\n",
    "    # First we need to open a new file with a descriptive name (sequence name seems like a good idea)\n",
    "    # Then we write the results in the file and don't forget to close it afterwards.\n",
    "    # In the end we also close the BLAST result file, before we go on to the next sequence.\n",
    "    save_file = open(seq_record.id + '_BLAST.xml', 'w')\n",
    "    save_file.write(result_handle.read())\n",
    "    save_file.close()\n",
    "    result_handle.close()   \n",
    "# This script can run a while, depending on the no. of sequences in your file and your hitlist_size value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Content](#Content)\n",
    "\n",
    "<a id='genbank'></a>\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "###Download Information from GenBank###\n",
    "---\n",
    "\n",
    "Before we blast our hits against the Arabidopsis genome, we extract information out these hits from GenBank and write these information into a CSV file. Afterwards we extract the amino acid sequences from each hit and write a FASTA file for the reciprocal BLAST.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AT1G01060_BLAST.xml 1474\n",
      "AT2G25930_BLAST.xml 263\n",
      "AT2G40080_BLAST.xml 327\n",
      "AT2G46790_BLAST.xml 10000\n",
      "AT2G46830_BLAST.xml 1441\n",
      "AT3G09600_BLAST.xml 1488\n",
      "AT3G46640_BLAST.xml 5005\n",
      "AT5G02810_BLAST.xml 10000\n",
      "AT5G08330_BLAST.xml 877\n",
      "AT5G24470_BLAST.xml 10000\n",
      "AT5G61380_BLAST.xml 5173\n"
     ]
    }
   ],
   "source": [
    "for xmlfile in glob.glob('*.xml'):\n",
    "    \n",
    "    # Open the XML file for each protein and read them.\n",
    "    result_handle = open(xmlfile)\n",
    "    blast_record = NCBIXML.read(result_handle)\n",
    "\n",
    "    # Create a new text file, to store GenIndex numbers needed to download information from GenBank.\n",
    "    save_file = open(xmlfile + '.txt', 'w')\n",
    "\n",
    "    rec = 0\n",
    "\n",
    "    # Read alignment title, split after each >, b/c hits sometimes have multiple header, and write each header in a\n",
    "    # new line of the text file. Close everything in the end.\n",
    "    for alignment in blast_record.alignments:\n",
    "        for i in alignment.title.split(' >', alignment.title.count('>')):\n",
    "            save_file.write(i + '\\n')\n",
    "\n",
    "        rec += 1\n",
    "\n",
    "    save_file.close()\n",
    "    result_handle.close()\n",
    "    print xmlfile, rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "protein = 'CHE'\n",
    "# Create an empty list to store used GenIndexes.\n",
    "GI_list = []\n",
    "\n",
    "# Create a CSV file, write a header line and close is afterwards.\n",
    "save_file = open('%s_BLAST.csv'%protein, 'w')\n",
    "save_file.write('name,taxonomy,gene_name,gi,db,taxid,db_source,length,seq,create_date,last_update,source\\n')\n",
    "save_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Enter your email address so NCBI knows who you are.\n",
    "Entrez.email =\"schmelli@msu.edu\"\n",
    "\n",
    "# Open the newly created CSV file, now with the append function, to add information from GenBank for each hit.\n",
    "save_file = open('%s_BLAST.csv'%protein, 'a')\n",
    "\n",
    "# Open text file with the headers in each line and store each line as an element in a list.\n",
    "result_handle = open('%s_BLAST.txt'%protein,'r')\n",
    "lines = result_handle.readlines()\n",
    "\n",
    "# Open a text file used to store GenIndexes that can't be found.\n",
    "save_file_2 = open('%s_not_found.txt'%protein, 'a')\n",
    "\n",
    "# Loop over the list of headers and extract the GenIndex, that is usually stored at the second position.\n",
    "# Send the GenIndex to GenBank to retrieve information about organism name, taxonomy and taxonomy ID,\n",
    "# protein annotation, multiple protein GenIndexes, sequence, sequence length, and create/update dates.\n",
    "for line in lines:\n",
    "    idp = line.split('|',2)[1]\n",
    "    \n",
    "    # Check if GenIndex is already used to avoid duplications.\n",
    "    if idp not in GI_list:\n",
    "        \n",
    "        # Try to send GenIndex to Genbank. If you send GenIndexes to fast Genbank will delete requests. \n",
    "        # In those cases an HTTPError will occur. Just wait a couple of seconds and try again.\n",
    "        try:\n",
    "            handle = Entrez.efetch(db=\"protein\", id=idp, retmode=\"xml\")\n",
    "        except HTTPError:\n",
    "            time.sleep(20)\n",
    "            handle = Entrez.efetch(db=\"protein\", id=idp, retmode=\"xml\")\n",
    "        records = Entrez.read(handle, validate=False)\n",
    "\n",
    "        # Look for an organism name in the Genbank file, if there is nothing, write the GenIndex into a different file\n",
    "        # and continue with the next GenIndex.\n",
    "        try:\n",
    "            save_file.write(records[0][\"GBSeq_organism\"].replace(',',' ')+\",\")\n",
    "        except IndexError:\n",
    "            save_file_2.write(line)\n",
    "            continue\n",
    "            \n",
    "        # In case there is an organism name, save information about organism name, taxonomy, protein annotation, \n",
    "        # and multiple protein GenIndexes.\n",
    "        save_file.write(records[0][\"GBSeq_taxonomy\"].replace(',',' ')+\",\")\n",
    "        save_file.write(records[0][\"GBSeq_definition\"].replace(',',' ')+\",\")\n",
    "        save_file.write(records[0][\"GBSeq_other-seqids\"][1].replace(',',' ')+\"|,\")\n",
    "        save_file.write(records[0][\"GBSeq_other-seqids\"][0].replace(',',' ')+\"|,\")\n",
    "\n",
    "        found = 0\n",
    "        # Find the taxonomy ID. If there is no, write an empty field in the CSV file. Do the same for a possible\n",
    "        # genomic source of the protein sequence.\n",
    "        for i in range(4):\n",
    "            try:\n",
    "                if records[0][\"GBSeq_feature-table\"][0][\"GBFeature_quals\"][i]['GBQualifier_name'] == 'db_xref':\n",
    "                    save_file.write(records[0][\"GBSeq_feature-table\"][0][\"GBFeature_quals\"][i]['GBQualifier_value'].replace(',',' ')+\",\")\n",
    "                    found = 1\n",
    "                    break\n",
    "            except IndexError:\n",
    "                save_file_2.write(',')\n",
    "        if found == 0:\n",
    "            save_file.write(',')\n",
    "\n",
    "        try:\n",
    "            save_file.write(records[0][\"GBSeq_source-db\"].split(',',1)[0]+\",\")\n",
    "        except KeyError:\n",
    "            save_file.write(',')\n",
    "\n",
    "        # If everything worked so far, save information about the sequence, length, create/update date, and source.\n",
    "        save_file.write(records[0][\"GBSeq_length\"].replace(',',' ')+\",\")\n",
    "        save_file.write(records[0][\"GBSeq_sequence\"].upper()+\",\")\n",
    "        save_file.write(records[0]['GBSeq_create-date'].replace(',',' ')+\",\")\n",
    "        save_file.write(records[0]['GBSeq_update-date'].replace(',',' ')+\",\")\n",
    "        save_file.write(records[0]['GBSeq_source'].replace(',',' ')+\"\\n\")\n",
    "\n",
    "        # Wait again to make sure not many requests go per second to NCBI.\n",
    "        GI_list.append(idp)\n",
    "        time.sleep(1)\n",
    "    \n",
    "save_file.close()\n",
    "save_file_2.close()\n",
    "result_handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "proteins = ['CCA1','CHE','ELF3','ELF4','LHY1','LUX','PRR5','PRR7','PRR9',\n",
    "            'RVE8','TOC1']\n",
    "\n",
    "for protein in proteins:\n",
    "    \n",
    "    # Create a FASTA file.\n",
    "    save_file = open('%s_BLAST.faa'%protein, 'w')\n",
    "\n",
    "    # Open the new CSV file, skip the header and write a FASTA file using the information from the CSV file.\n",
    "    with open('%s_BLAST.csv'%protein, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        # Skip header\n",
    "        next(reader) \n",
    "        for row in reader:\n",
    "            # Write multiple GenIndexes, protein annotation, and organism.\n",
    "            save_file.write('>'+row[3].replace('||','|')+row[4].replace('||','|')+' '+row[2]+' '+row[0] + '\\n')\n",
    "            # Write sequence\n",
    "            save_file.write(row[8] + '\\n')\n",
    "\n",
    "    save_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Content](#Content)\n",
    "\n",
    "<a id='BLAST back'></a>\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "###Blast hits back against the genomes###\n",
    "---\n",
    "\n",
    "Next we will blast all the sequences from our first BLAST run back against the Arabidopsis genome to validate to correctness of our first run. Only hits that align to the original protein are considered a 'real' hit.\n",
    "\n",
    "e.g.: If you first blasted TOC1 against the 'nr' database. You would blast every hit you got in this analysis back against the Arabidopsis genome. Now you get for every hit, just one hit in the genome. Next you would filter only these hits that match with TOC1.\n",
    "\n",
    "I ran this analysis using the standalone local BLAST from the command line, b/c I had problems with the NCBI server. Installation information for BLAST can be found [here](http://blast.ncbi.nlm.nih.gov/Blast.cgi?PAGE_TYPE=BlastDocs&DOC_TYPE=Download). Following is the command line script you need to run for reproduction of my results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Arabidopsis thaliana###\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Command line code for standalone local BLAST run\n",
    "\n",
    "# For more information about the blastp options\n",
    "blastp -help\n",
    "\n",
    "# Build your own BLAST database from the genome file\n",
    "# -in: Input fasta file\n",
    "# -dbtype: Database type, either nucleotide or protein\n",
    "# -out: Database name\n",
    "makeblastdb -in Arabiodopsis_genome_prot.faa -dbtype 'prot' -out Arabiodopsis_genome_prot\n",
    "\n",
    "# Run BLAST for a set of input files\n",
    "# -query: Input file\n",
    "# -db: Database of your BLAST run\n",
    "# -out: Output file name\n",
    "# -evalue: Expectation value (E) threshold for saving hits. Default = 10\n",
    "# -word_size: Word size for wordfinder algorithm\n",
    "# -outfmt: alignment view options:\n",
    "     0 = pairwise,\n",
    "     1 = query-anchored showing identities,\n",
    "     2 = query-anchored no identities,\n",
    "     3 = flat query-anchored, show identities,\n",
    "     4 = flat query-anchored, no identities,\n",
    "     5 = XML Blast output,\n",
    "     6 = tabular,\n",
    "     7 = tabular with comment lines,\n",
    "     8 = Text ASN.1,\n",
    "     9 = Binary ASN.1,\n",
    "    10 = Comma-separated values,\n",
    "    11 = BLAST archive format (ASN.1) \n",
    "    12 = JSON Seqalign output\n",
    "# -num_alingments: Number of database sequences to show alignments for. Default = 250\n",
    "\n",
    "for i in seq/*.faa \n",
    "do \n",
    "blastp -query \"$i\" -db Arabiodopsis_genome_prot -out \"${i/.faa}\".xml -evalue 10 -word_size 3 -outfmt 5 -num_alignments 1\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[Back to Content](#Content)\n",
    "\n",
    "<a id='filter'></a>\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "###Filtering of the reciprocal BLAST###\n",
    "---\n",
    "\n",
    "After finishing the reciprocal BLAST, we filtered these hits, that align to original input protein. For this the GenIndex numbers for each protein is needed. We took the numbers from the corresponding genome file. Only those hits that best align to the orginial protein were retained for further analyses. The BLAST results of the remaining hits were used to write a CSV file. A CSV file represents a file format easier to work with for further analyses and visualization using the python package __pandas__.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Genbank identifiers (GI) for all 11 proteins\n",
    "CCA1 = '30690518'\n",
    "CHE = '15241596'\n",
    "ELF3 = '15225220'\n",
    "ELF4 = '18405258'\n",
    "LHY1 = '145323696'\n",
    "LUX = '334185766'\n",
    "PRR5 = '18420797'\n",
    "PRR7 = '18414032'\n",
    "PRR9 = '18407171'\n",
    "RVE8 = '30680926'\n",
    "TOC1 = '15240235'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.71828182846 no. of records: 2472\n",
      "2.71828182846 no. of records: 1351\n",
      "2.71828182846 no. of records: 403\n",
      "2.71828182846 no. of records: 674\n",
      "2.71828182846 no. of records: 2543\n",
      "2.71828182846 no. of records: 7425\n",
      "2.71828182846 no. of records: 24376\n",
      "2.71828182846 no. of records: 27573\n",
      "2.71828182846 no. of records: 21818\n",
      "2.71828182846 no. of records: 2503\n",
      "2.71828182846 no. of records: 10517\n"
     ]
    }
   ],
   "source": [
    "proteins = ['CCA1','CHE','ELF3','ELF4','LHY1','LUX','PRR5','PRR7','PRR9',\n",
    "            'RVE8','TOC1']\n",
    "\n",
    "GI = [CCA1,CHE,ELF3,ELF4,LHY1,LUX,PRR5,PRR7,PRR9,RVE8,TOC1]\n",
    "\n",
    "\n",
    "for prot,gi in zip(proteins,GI):\n",
    "\n",
    "    # Read the BLAST XML file and load it into the NCBIXML.parser.\n",
    "    result_handle = open('%s_BLAST.xml'%prot)\n",
    "    blast_records = NCBIXML.parse(result_handle)\n",
    "\n",
    "    # Open a CSV file and write the header in the first line.\n",
    "    csv = open('%s_hits.csv'%prot,'w')\n",
    "    csv.write('query_name,query_organism,query_strain,query_gene,query_gi,query_length,e_value,bitscore,identity,database_species,database_gene,database_gi,database_gene_length\\n')\n",
    "\n",
    "    records = 0\n",
    "\n",
    "    # Loop over every record in your XML and count them.\n",
    "    for blast_record in blast_records:\n",
    "        records += 1\n",
    "\n",
    "        # Check the GI for the alignment. If the hit matches your protein of interest write an entry in the CSV file.\n",
    "        for alignment in blast_record.alignments:\n",
    "            if gi in alignment.title.lower():\n",
    "\n",
    "                # Some hits contain 'MULTISPECIES' entries. Loop over each organism in the query title.\n",
    "                for i in blast_record.query.split(' >', blast_record.query.count('>')):\n",
    "                    # Organisms names are enclosed by square bracktes and we only want to keep these hits.\n",
    "                    if '[' in i:\n",
    "                        org_name = i.split('[',1)[1]\n",
    "                        gen_name = i.split(' ',1)[1]\n",
    "\n",
    "                        # Extract name\n",
    "                        csv.write(org_name[0:org_name.find(']')].replace(',',' ').replace(\"'\",'').replace('[','').replace(']','').lstrip())\n",
    "                        csv.write(',')\n",
    "\n",
    "                        # Extract species name\n",
    "                        csv.write(org_name[0:org_name.find(']')].replace(',',' ').replace(\"'\",'').replace('[','').replace(']','').lstrip().split(' ',i.split('[',1)[1][0:-1].count(' '))[0])\n",
    "                        csv.write(',')\n",
    "\n",
    "                        # Extract strain name\n",
    "                        csv.write(' '.join(org_name[0:org_name.find(']')].replace(',',' ').replace('[','').replace(']','').lstrip().split(' ',i.split('[',1)[1][0:-1].count(' '))[1:]))\n",
    "                        csv.write(',')\n",
    "\n",
    "                        # Extract gene name\n",
    "                        csv.write(str(gen_name[0:gen_name.find(' [')].replace(',',' ')))\n",
    "                        csv.write(',')\n",
    "\n",
    "                        # Extract GI of that gene\n",
    "                        csv.write(str(i.split('|',2)[1]))\n",
    "                        csv.write(',')\n",
    "                        for hsp in alignment.hsps:\n",
    "\n",
    "                            # Extract gene length\n",
    "                            csv.write(str(blast_record.query_length))\n",
    "                            csv.write(',')\n",
    "\n",
    "                            # Extract e-value, bitscore, identity of that hit\n",
    "                            csv.write(str(hsp.expect)+','+str(hsp.score)+','+str(((hsp.identities/float(len(hsp.match)))*100)))\n",
    "                            csv.write(',')\n",
    "\n",
    "                            # Extract SyPCC7942 or SyPCC6803 organims name\n",
    "                            csv.write(alignment.title.split('[',1)[1][0:-1])\n",
    "                            csv.write(',')\n",
    "\n",
    "                            # Extract SyPCC7942 or SyPCC6803 gene name\n",
    "                            csv.write(alignment.title.split('|',7)[6].split('[',1)[0])\n",
    "                            csv.write(',')\n",
    "\n",
    "                            # Extract SyPCC7942 or SyPCC6803 GI for that gene\n",
    "                            csv.write(alignment.title.split('|',4)[3])\n",
    "                            csv.write(',')\n",
    "\n",
    "                            # Extract SyPCC7942 or SyPCC6803 gene length\n",
    "                            csv.write(str(alignment.length))\n",
    "                            csv.write('\\n')\n",
    "\n",
    "                            break\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "    csv.close() \n",
    "\n",
    "    print prot,'no. of records:',records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Content](#Content)\n",
    "\n",
    "<a id='CSV'></a>\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "###Merging and final filtering###\n",
    "---\n",
    "\n",
    "In the last step in the collection and the preprocessing of the raw data, we merged the two CSV file (The one with the GenBank information of each hit, and the one with the filtered hits of the reciprocal best hit analysis). After merging the two files only those records that had a complete set of information were retained for further analyses. Resulting in the end in a set of CSV files, where each protein is represented by one CSV file containing all information.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('mode.chained_assignment',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function that merges two CSV files using pandas\n",
    "def merge_csv(path1, path2):\n",
    "    \n",
    "    # Read the first CSV file and remove all rows with incomplete information. Take a set of columns and rename them.\n",
    "    df_1 = pd.read_csv(path1, index_col=False)\n",
    "    df_1 = df_1.dropna()\n",
    "    a = df_1[['name','taxonomy','taxid','gene_name','db','length','seq','db_source',\n",
    "              'create_date','last_update']]\n",
    "    a.columns = ['query_name','taxonomy','taxid','gene_name','db','query_length','seq',\n",
    "                 'db_source','create_date','last_update']\n",
    "    \n",
    "    # Read second CSV file and remove all rows with incomplete information.\n",
    "    df_2 = pd.read_csv(path2, index_col=False)\n",
    "    df_2 = df_2.dropna()\n",
    "    b = df_2[['query_name','query_organism','query_gi','query_length','e_value','bitscore',\n",
    "              'identity']]\n",
    "    \n",
    "    # Merge the two CSV files.\n",
    "    c = pd.merge(a,b, how='left')\n",
    "    \n",
    "    # Remove again all rows with incomplete information, just to make sure the merging did not create any NaN values.\n",
    "    # Next retain all hits that have an genome source, that contains the word 'accession'. Remove all duplicates.\n",
    "    # In the end reset the index, and reorder and rename columns.\n",
    "    c = c.dropna()\n",
    "    c = c[c.db_source.str.contains('accession')]\n",
    "    c = c.drop_duplicates(subset=['query_name','seq'])\n",
    "    c.index = range(1,len(c)+1)\n",
    "    c = c[['query_name','query_organism','taxonomy','taxid','gene_name','db','query_gi',\n",
    "           'db_source','query_length','e_value','bitscore','identity','seq','create_date',\n",
    "           'last_update']]\n",
    "    \n",
    "    name = path1.split('_',1)[0]\n",
    "    \n",
    "    c.columns = ['query_name','query_organism','taxonomy','taxid','gene_name','db',name+'_gi',\n",
    "                 'db_source',name+'_length','e_value','bitscore','identity','seq',\n",
    "                 'create_date','last_update']\n",
    "    \n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the merge CSV file for every single protein\n",
    "proteins = ['CCA1','CHE','ELF3','ELF4','LHY1','LUX','PRR5','PRR7','PRR9',\n",
    "            'RVE8','TOC1']\n",
    "\n",
    "for i in proteins:\n",
    "    \n",
    "    abc = merge_csv('%s_BLAST.csv'%i,\n",
    "                      '%s_hits.csv'%i)\n",
    "\n",
    "    abc.to_csv('%s_all.csv'%i, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###Further analyses###\n",
    "---\n",
    "\n",
    "All of the analyses performed using this set of CSV files created in the code above can be found in the following IPython notebook.\n",
    "\n",
    "+ [Distribution of circadian clock proteins from plants](2_Plant_Clock_Heatmap.ipynb)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
